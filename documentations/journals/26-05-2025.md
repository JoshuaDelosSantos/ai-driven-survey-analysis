# Focus
2. Core Topics Identified for Research
- 2.1 Initial topics:
- Sentiment Analysis
- Retrieval-Augmented Generation (RAG)
- Database technologies
- Data Privacy
- LLM selection
- 2.2 Expanded topics (from today's discussion and the detailed research prompt we curated):
- Sentiment Analysis Deep Dive: Fundamentals (rule-based, ML, hybrid), specific models/tools (OpenAI API, local options like RoBERTa), survey-specific preprocessing, handling nuances (sarcasm, Aspect-Based Sentiment Analysis), RAG integration, evaluation metrics, and ethical considerations.
- Embedding strategies
- Chunking and indexing for RAG 
- Data normalization and schema design
- File ingestion automation (cloud-hosted workflows, specifically Google Sheets API vs. Drive API for Qualtrics data )
- Stakeholder-facing presentation design
- API usage constraints (e.g. OpenAI rate limits, privacy implications)earning, Refining Technical Strategy for Sentiment Analysis & Data Handling in MVP.

1. Project Scope and Learning Outcomes
- 1.1 I refined my initial learning outcomes to be more flexible and skill-oriented, based on feedback from my subject coordinator.
- 1.2 I aligned outcomes around broad capabilities such as applying modern AI techniques, stakeholder communication, and ethical considerations.
- 1.3 I re-centered the project goals around outcomes (skills and learning) rather than rigid outputs.

2. Core Topics Identified for Research
* 2.1 Initial topics:
* Sentiment Analysis
* Retrieval-Augmented Generation (RAG)
* Database technologies
* Data Privacy
* LLM selection
* 2.2 Expanded topics (from today’s discussion and the detailed research prompt we curated):
* Sentiment Analysis Deep Dive: Fundamentals (rule-based, ML, hybrid), specific models/tools (OpenAI API, local options like RoBERTa), survey-specific preprocessing, handling nuances (sarcasm, Aspect-Based Sentiment Analysis), RAG integration, evaluation metrics, and ethical considerations.
* Embedding strategies
* Chunking and indexing for RAG 
* Data normalization and schema design
* File ingestion automation (cloud-hosted workflows, specifically Google Sheets API vs. Drive API for Qualtrics data )
* Stakeholder-facing presentation design
* API usage constraints (e.g. OpenAI rate limits, privacy implications)


3. Technical Architecture & Workflow Design
- 3.1 I chose an MVP architecture using:
- OpenAI API (for embeddings and generation )
- PostgreSQL (relational DB for normalized storage )
- Docker (to orchestrate modular services )
- 3.2 Original Workflow design:
- Survey responses are collected via Qualtrics and saved to a cloud Excel file (e.g. Google Drive).
- A Dockerized worker service will periodically fetch this file, transform it using ETL logic, and populate normalized database tables.
- Free-text feedback will be cleaned, chunked, embedded using OpenAI, and enriched with sentiment scores.
- RAG will then retrieve and summarize relevant responses in real-time for user queries via a front-end or API layer.
- 3.3 Decision for MVP1 - Manual DB Population:
- To focus development, I've decided to skip live Google Sheet ingestion for MVP1 and will populate the database manually with an initial 100 survey responses.
- Learned methods for manual DB populating (PostgreSQL in Docker):
- Preparing data in CSVs for users, modules, and responses tables.
- Using the COPY command (recommended): docker cp CSVs to the db container, then psql to execute COPY FROM.
- SQL INSERT statements: More manual but direct.
- Database GUI Tool: Requires port mapping for the db service.
- 3.4 Sentiment Analysis Workflow (with manually populated DB):
- The worker component will identify responses in the responses table needing processing.
- It will then perform text cleaning and chunking.
- Embeddings will be generated for each chunk using the OpenAI Embeddings API.
- Sentiment analysis will be performed on these chunks.
- Results (chunk text, embedding, sentiment score) will be stored in the response_chunks table.
- 3.5 Refining Sentiment Analysis - Local RoBERTa:
- Comparison: OpenAI Sentiment API vs. Local Hugging Face RoBERTa:
- OpenAI: Easier initial setup, good general performance, very low cost for 100 rows, but data sent externally.
- RoBERTa (local): More setup (libraries, model download), high data privacy, free model (compute cost for local execution), potential for customisation, offline capability.
- Decision for MVP1: Decided to refactor the architecture to use a local RoBERTa model for sentiment analysis (keeping OpenAI for embeddings). This enhances data privacy for sentiment processing and builds local ML capability.
- Architectural Refactor for RoBERTa:
- The worker component's definition in the architecture document now specifies it "Runs local RoBERTa model for sentiment analysis".
- The worker's Dockerfile will need to include transformers, torch, and the RoBERTa model files. This will increase the image size and resource requirements (CPU/RAM) for the worker.
- The "Asynchronous Processing" step for sentiment analysis was updated to reflect loading the local RoBERTa model, tokenizing text, performing inference, and processing logits to get scores.
- Implementation Choice for RoBERTa: Direct Model vs. pipeline():
- Clarified that the Hugging Face pipeline() is a high-level utility that uses models like RoBERTa.
- pipeline("sentiment-analysis", model="roberta-model..."): Easiest and quickest to implement, less code, good for standard tasks.
- Direct model usage (AutoModel..., AutoTokenizer...): Maximum control and flexibility, more code, steeper learning curve.
- Recommendation for MVP: Start with the pipeline() utility for simplicity. Can refactor to direct model usage later if more control is needed.


4. Practical Considerations and Design Nuances
- 4.1 Data ingestion (Learnings for future iteration, post-MVP1):
- I learned that Qualtrics updates a single file rather than creating new ones.
- This requires a polling or scheduled retrieval approach rather than file-watching. Reviewed three approaches: Google Sheets API (recommended), Google Drive API (download XLSX), and rclone/gdrive-fuse (less ideal).
- Google Sheets API Polling Details:
- Requires a service account, enabling the API in GCP, and sharing the Sheet with the service account's email.
- The worker runs a Python script (e.g., scheduled with APScheduler) to fetch data, identify new rows (vs. max response_id or timestamp already in DB), and upsert them.
- 4.2 ETL approach:
- I’ll implement a deduplication or upsert strategy based on a unique response_id or submitted_at field.
- I now understand the importance of maintaining idempotency and tracking previously ingested data.
- 4.3 Privacy and constraints:
- I identified the need to research data privacy concerns (e.g., GDPR). Using local RoBERTa for sentiment addresses one aspect of this for the survey text.
- I also acknowledged that LLM choice (for RAG generation) might depend on latency, cost, rate limits, and trust boundaries (e.g., open-source vs. cloud APIs).


5. Mindset and Reflection
- 5.1 I’ve moved from a tool-centric mindset to an outcome-driven one, focusing on what I want to learn and demonstrate—not just what I build.
- 5.2 I feel more confident now about how to integrate cloud-stored data into a local processing pipeline (even if deferred for MVP1) and also how to run ML models locally within my Dockerized architecture.
- 5.3 I identified communication and public speaking as skill gaps and tied those into the project as learning goals.
- 5.4 I welcomed intellectual challenge throughout the process today and practiced mapping high-level ideas to detailed architecture, including making decisions about significant architectural changes (like local vs. API-based sentiment analysis).