# Journal Entry

**Focus:** Setting up Database (Tables and Content) & Sentiment Analysis Module

## Reflecting on Previous Progress

Yesterday was quite productive. I successfully set up the Docker container and established the initial database connection. A key achievement was creating the scripts for the user table in the database; these scripts are designed to gracefully handle pre-existing tables by logging the situation and moving on. To populate this, I imported mock user data into the repository and developed a script to load this data into the database.

## Today's Developments and Challenges

The primary focus today shifted towards creating more scripts, particularly for the sentiment analysis module, alongside finalising its initial setup. To deepen my understanding of connecting to PostgreSQL with Python, I watched a helpful [Postgres Tutorial on YouTube](https://www.youtube.com/watch?v=2PDkXviEMD0).

Key activities and accomplishments for today include:

- **Database Expansion:** I created scripts to establish the `learning_content` table and subsequently loaded it with data from a CSV file.
- **AI Tool Utilisation:**
    - I prompted Gemini 2.5 Pro to generate a data dictionary in JSON format for my mock data. This dictionary will be valuable for providing context to an LLM for future queries.
    - Claude 3.7 Sonnet was instrumental in generating Python scripts for creating the `attendance` and `evaluation` tables, as well as the scripts needed to load data into these respective tables.
- **Sentiment Analysis Module Setup:** A major achievement today was setting up the foundational structure of the `sentiment-analysis` module. This module is designed to perform sentiment analysis on free-text survey data using the Hugging Face RoBERTa model (`cardiffnlp/twitter-roberta-base-sentiment`) and store results back in the database. The architecture is class-based for maintainability and testability:
    - **`config.py`**: Centralises all configuration constants such as the model name, database URI, table names, and the specific free-text columns targeted for analysis.
    - **`analyser.py`**: Defines a `SentimentAnalyser` class responsible for loading the Hugging Face tokenizer and model, and providing an `analyse` method to return sentiment scores (negative, neutral, positive) for a given text.
    - **`db_operations.py`**: Contains a `DBOperations` class that handles database interactions. It establishes a connection using credentials from `config.py` and includes a method to upsert sentiment scores into a dedicated table.
    - **`data_processor.py`**: Introduces a `DataProcessor` class. This class accepts instances of `DBOperations` and `SentimentAnalyser` and orchestrates the main workflow: fetching evaluation rows, iterating over configured free-text fields, calling the analysis function, and then persisting the results using `DBOperations`.
    - **`runner.py`**: Serves as the main script entry point. It ensures the necessary sentiment table exists (by invoking `src/db/create_sentiment_table.py`), parses command-line arguments if any, instantiates the core classes (`SentimentAnalyser`, `DBOperations`, `DataProcessor`), and finally calls `DataProcessor.process_all()` to execute the entire sentiment analysis pipeline.
- **Documentation & Design:** I updated the `db/README.md` file to reflect the recent changes and additions. The architecture definition for the sentiment analysis module, as outlined above, was also finalised.
- **Technical Hurdles:** A small portion of time was spent troubleshooting issues with Python environments and imports. Debugging involved carefully analysing error messages, freezing pip dependencies to identify discrepancies, updating the `requirements.txt` file, and, at times, creating new Python virtual environments to ensure a clean state.
- **Work for tommorow** I ran `runner.py` to test its functionality, it works! However, I noticed that my mock data was inconsistent (created GitHub issue).
    - Add testing for sentiment-analysis

### Key Learnings from Video: [Postgres Tutorial - YouTube](https://www.youtube.com/watch?v=2PDkXviEMD0)

The tutorial offered valuable insights into working with PostgreSQL using Python, specifically the `psycopg2` library:

- **Connecting to PostgreSQL:** The video detailed the process of establishing a connection by providing necessary parameters (host, database name, user, password) and importantly, stressed closing the connection afterwards to free up resources.
- **Using Cursors:** It highlighted that cursors, created from the connection object, are essential for executing SQL commands.
- **Reading Data (SELECT):** SQL queries are executed via `cursor.execute()`. The video demonstrated retrieving results using methods like `cursor.fetchall()` and iterating through them.
- **Writing Data (INSERT):** The `cursor.execute()` method is also employed for `INSERT` statements. A critical best practice emphasised was the use of parameterised queries (e.g., using `%s` as placeholders) to prevent SQL injection vulnerabilities.
- **Committing Changes:** For data modification operations (like `INSERT`, `UPDATE`, `DELETE`), it's crucial to save these changes to the database using `connection.commit()`.
- **Error Handling:** The video implicitly demonstrated error handling by showing the database's reaction to an attempt to insert a duplicate primary key, which serves as a good illustration of how data integrity is maintained.
- **Best Practices Reinforced:**
    - Always ensure connections and cursors are closed after use.
    - Exercise caution with unbounded queries on large tables (e.g., `SELECT *` without appropriate `LIMIT` or `WHERE` clauses).
    - Consistently use parameterised queries to safeguard against SQL injection.
    - Remember to commit transactions when data has been modified.