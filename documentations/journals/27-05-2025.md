# Focus
1. [RAG using PG Vector scale and Python - YouTube](https://www.youtube.com/watch?v=hAdEuDBN57g)
- 1.1 PG Vector Scale & Python Setup: The video provides a walkthrough on establishing a high-performance RAG solution by utilising PG Vector scale in conjunction with Python. [00:00]
- 1.2 PostgreSQL as a Vector Database: It explains how PostgreSQL, when augmented with the PG Vector extension, can function effectively as a database for storing vector embeddings. [01:00]
- 1.3 Understanding PG Vector Scale: The video clarifies that PG Vector scale, a product from Timescale, enhances PG Vector by introducing a new index. This addition is designed to accelerate query result retrieval. [03:53]
- 1.4 Docker Environment Configuration: Viewers are shown how to employ Docker Compose to configure the essential environment for the PostgreSQL database, specifically using the TimescaleDB image. [06:24]
- 1.5 Database Connection: The process of connecting to the PostgreSQL database is demonstrated, using a GUI client such as TablePlus as an example. [09:08]
- 1.6 Vector Insertion: The video illustrates the use of a Python script to process an FAQ dataset. This involves vectorising the text data with an embedding model (specifically OpenAI's) and subsequently storing these generated vectors within the PostgreSQL database. [10:18]
- 1.7 Similarity Search Functionality: The concept of performing a similarity search on the stored vector embeddings is explained. This allows for the retrieval of the most relevant results in response to a user's query. [14:48]
- 1.8 AI-Powered Answer Generation: The video demonstrates how the context retrieved from the similarity search can be used in conjunction with an AI model (OpenAI) to synthesise answers to user questions. [22:45]
- 1.9 Metadata Filtering: The importance and method of filtering search results based on metadata (e.g., category) associated with the vector embeddings are highlighted. [26:55]
- 1.10 Advanced Filtering with Predicates: The video introduces the use of predicates to implement more intricate conditional logic for refining search results based on a variety of criteria. [28:35]
- 1.11 Time-Based Filtering: A technique for filtering search results based on a specific time range is explained. This leverages the timestamp embedded within the UUID of the records. [31:14]
- 1.12 Developing High-Performance RAG Systems: Beyond fundamental vector search, the video touches upon the construction of more sophisticated RAG systems. This includes the incorporation of language models, structured output, and hybrid search capabilities. [32:57]

2. [Building a Retrieval Augmented Generation (RAG) application - YouTube](https://www.youtube.com/watch?v=NC4msKJ_Euo)
- 2.1 Retrieval Augmented Generation (RAG): This is an approach to improve LLM answer accuracy by feeding relevant external information and then generating answers based on that context. [00:18]
- 2.2 RAG Application Workflow: The process begins when a user asks a question [01:14], searches a database/search engine [01:21], retrieves relevant results [01:28], sends question+results to an LLM with a system prompt [01:44], and receives an answer [02:05].
- 2.3 Search Engine Importance: Ideally supports both full-text and vector (semantic) search for comprehensive retrieval. [02:53]
- 2.4 Postgres and PG Vector Extension: Recommended for its simple setup and suitability for local testing. [03:30]
- 2.5 Vector Embeddings: Text (titles/descriptions) → numerical vectors [14:51,15:11], enabling semantic search [16:51]; models generate embeddings (hosted or local) [17:51,18:08]; ensure same model/dimensions for embedding & search [18:48,41:45].
- 2.6 Full-Text Search in Postgres: Uses tsvector, tsquery, ts_rank_cd for keyword-based search. [23:34,24:39]
- 2.7 Hybrid Search: Combines vector and full-text search to leverage both strengths. [42:57,43:17]
- 2.8 Reciprocal Rank Fusion (RRF): Merges results from different search strategies to improve ranking. [43:39]
- 2.9 Reranking Models: Specialized models to reorder initial search results based on relevance. [44:39]
- 2.10 Interacting with LLMs: Uses Azure AI Inference SDK for marketplace models [34:35,57:36]; format retrieved results in Markdown [57:13]; include a system prompt with source‐citation instructions [01:00:21]; control randomness via temperature [58:20].
- 2.11 Importance of Retrieval Quality: Fundamental to RAG application success. [31:32]
- 2.12 Query Rewriting: Use an LLM to refine user queries for better retrieval accuracy. [55:37]
- 2.13 Handling Aggregate Queries: Basic RAG may not suit dataset‐wide analysis—consider NLP→SQL or Pandas/NumPy + LLM. [01:08:01]
- 2.14 Debugging RAG Applications: Distinguish poor retrieval issues from LLM-context limitations. [01:12:30]
- 2.15 Evaluation of RAG Applications: Verify LLM correctly uses sources and admits unknowns. [01:01:59]

3. Researched about RAG integration with survey data.
- 3.1 Well-designed PostgreSQL schema: normalisation, appropriate data types, foreign keys, and indexing.
- 3.2 Relational data for LLMs (Text-to-SQL): provide schema via DDL comments, structured descriptions (JSON/YAML), or knowledge graphs.
- 3.3 Embedding strategies for survey data: unstructured text → dense vector embeddings.
- 3.4 Text preprocessing: lowercasing, tokenisation, punctuation handling, stop-word removal (with caution), stemming/lemmatisation, and filtering survey-specific noise (boilerplate, typos, slang, emojis).
- 3.5 Chunking strategies: fixed-size, sentence-based, or semantic methods to manage LLM context windows and improve retrieval.
- 3.6 pgvector extension: store and search high-dimensional vectors in PostgreSQL for semantic search.
- 3.7 Sentiment-aware retrieval: filter and re-rank retrieved chunks using sentiment labels and scores.
- 3.8 LLM-driven Text-to-SQL: leverage LLMs with schema representation, few-shot examples, and clear prompts.
- 3.9 SQL validation & security: syntactic/semantic checks on LLM-generated queries and enforce strict DB permissions.
- 3.10 Sentiment analysis fine-tuning: adapt pre-trained Transformer models on domain-specific survey data.
- 3.11 Aspect-Based Sentiment Analysis (ABSA): identify opinions on specific aspects or features.
- 3.12 Sentiment in RAG prompts: supply retrieved chunks with sentiment labels, scores, and aspect info for summarisation.
- 3.13 Orchestration frameworks: use LangChain, LlamaIndex, etc., for pre-built RAG components.
- 3.14 Performance evaluation: assess retrieval, generation, Text-to-SQL metrics, and end-to-end human evaluation.
- 3.15 Drift monitoring & retraining: track data/concept drift and retrain sentiment models as needed.
- 3.16 Schema evolution management: ensure Text-to-SQL stays functional as your PostgreSQL schema changes.
- 3.17 Ethical considerations: address model bias and data privacy throughout the system lifecycle.

# Research References
