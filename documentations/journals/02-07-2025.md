# Journal Entry - 2 July 2025

**Focus:** Black-box Testing Analysis & Query Logic Issue Resolution Plan

## **Testing Summary: Critical Query Logic Issue Identified**

### **Issue Discovery Through Black-box Testing**
* Conducted comprehensive black-box testing of the RAG terminal application
* Identified a critical query logic flaw affecting user feedback queries about learning content
* **Root Cause**: LLM generating incorrect SQL joins between wrong tables

---

## **Problem Analysis**

### **The Query Logic Error**

**User Query**: "What feedback did users give about 'Live Learning' type learning contents?"

**Incorrect SQL Generated by LLM**:
```sql
SELECT
  ruf.comment AS user_comment,
  ruf.rating AS user_rating,
  lc.name AS learning_content_name
FROM rag_user_feedback AS ruf
JOIN learning_content AS lc
  ON ruf.query_text LIKE '%' || lc.name || '%'
WHERE
  lc.content_type = 'Live Learning'
```

**Why This is Wrong**:
- `rag_user_feedback` contains feedback about **RAG system performance** (queries like "How many users completed training?")
- `learning_content` contains course information
- **No semantic relationship** exists between RAG system queries and learning content names
- Results in 0 rows returned → Answer synthesis failure

**Correct SQL Should Be**:
```sql
SELECT
  e.general_feedback,
  e.did_experience_issue_detail,
  e.course_application_other,
  lc.name AS learning_content_name,
  e.positive_learning_experience,
  e.effective_use_of_time,
  e.relevant_to_work
FROM evaluation e
JOIN learning_content lc ON e.learning_content_surrogate_key = lc.surrogate_key
WHERE lc.content_type = 'Live Learning'
  AND (e.general_feedback IS NOT NULL 
       OR e.did_experience_issue_detail IS NOT NULL 
       OR e.course_application_other IS NOT NULL)
```

### **Data Model Confusion**
The LLM lacks clear understanding of:
- **`rag_user_feedback`**: Feedback about the RAG system itself (meta-feedback)
- **`evaluation`**: Feedback about learning content and courses (actual content feedback)

---

## **Implementation Plan: Enhanced Schema Context System**

### **Phase 1: Immediate Fix - Enhanced Schema Descriptions**

#### **1.1 Update Schema Manager Context Generation**
**File**: `src/rag/core/text_to_sql/schema_manager.py`

**Objective**: Provide clearer table purpose descriptions to the LLM

**Implementation**:
```python
def _get_enhanced_table_descriptions(self) -> Dict[str, str]:
    """
    Enhanced table descriptions with clear purpose and usage context.
    """
    return {
        "users": "Contains user demographic information (user_level, agency). Use for demographic analysis and user categorization.",
        
        "learning_content": "Contains learning course/content metadata (name, content_type, target_level). Use for content analysis and categorization.",
        
        "attendance": "Contains user enrollment and completion records. Use for attendance statistics, completion rates, and participation analysis.",
        
        "evaluation": "Contains POST-COURSE EVALUATION RESPONSES from users about their learning experience. Use for USER FEEDBACK about learning content, satisfaction ratings, and course quality assessment. Contains free-text fields: general_feedback, did_experience_issue_detail, course_application_other.",
        
        "rag_embeddings": "Contains vector embeddings of evaluation free-text responses for semantic search. Internal system table for vector operations.",
        
        "rag_user_feedback": "Contains FEEDBACK ABOUT THIS RAG SYSTEM ITSELF (not about learning content). Users rate the quality of RAG system responses. Do NOT use for learning content feedback queries."
    }
```

#### **1.2 Enhanced Relationship Documentation**
```python
def _get_table_relationships_context(self) -> str:
    """
    Provide clear relationship context for SQL generation.
    """
    return """
    KEY TABLE RELATIONSHIPS:
    
    FOR LEARNING CONTENT FEEDBACK QUERIES:
    - evaluation → learning_content (via learning_content_surrogate_key)
    - evaluation contains actual user feedback about courses/content
    - Use evaluation.general_feedback, evaluation.did_experience_issue_detail, evaluation.course_application_other
    
    FOR RAG SYSTEM PERFORMANCE QUERIES:
    - rag_user_feedback contains feedback about this AI system's responses
    - Do NOT use rag_user_feedback for learning content questions
    
    FOR ATTENDANCE/PARTICIPATION QUERIES:
    - attendance → users (via user_id)
    - attendance → learning_content (via learning_content_surrogate_key)
    """
```

### **Phase 2: Query Classification Enhancement**

#### **2.1 Add Table-Specific Classification Patterns**
**File**: `src/rag/core/routing/query_classifier.py`

**Enhancement**: Add patterns that specifically route feedback queries correctly

```python
_feedback_routing_patterns = {
    "evaluation_feedback": [
        r'\b(?:feedback|comments|experience|satisfaction).*(?:about|regarding|for).*(?:course|content|learning|training)\b',
        r'\b(?:user|participant|student).*(?:feedback|comments|experience).*(?:course|content|learning)\b',
        r'\b(?:what.*said|opinions.*about|thoughts.*on).*(?:course|content|learning)\b',
        r'\b(?:course|content|learning).*(?:feedback|comments|experience|satisfaction|quality)\b'
    ],
    "rag_system_feedback": [
        r'\b(?:feedback|comments).*(?:about|regarding).*(?:system|response|answer|query|question)\b',
        r'\b(?:rag|system).*(?:feedback|performance|quality)\b',
        r'\b(?:how.*good|quality.*of).*(?:response|answer|system)\b'
    ]
}
```

#### **2.2 Enhanced SQL Generation Prompts**
**File**: `src/rag/core/text_to_sql/sql_tool.py`

**Enhancement**: Add specific guidance for feedback queries

```python
FEEDBACK_QUERY_GUIDANCE = """
CRITICAL: When users ask about feedback/comments/experiences about LEARNING CONTENT:
- Use the 'evaluation' table (contains course feedback)
- Join evaluation → learning_content via learning_content_surrogate_key
- Use evaluation.general_feedback, evaluation.did_experience_issue_detail, evaluation.course_application_other
- Include satisfaction ratings: positive_learning_experience, effective_use_of_time, relevant_to_work

NEVER use 'rag_user_feedback' for learning content feedback queries.
rag_user_feedback is only for feedback about this AI system itself.
"""
```

### **Phase 3: Enhanced Error Detection & Recovery**

#### **3.1 Query Validation Layer**
**File**: `src/rag/core/text_to_sql/query_validator.py` (new)

**Objective**: Detect and correct common query logic errors

```python
class QueryLogicValidator:
    """
    Validates SQL queries for logical correctness and suggests corrections.
    """
    
    def __init__(self):
        self.problematic_patterns = [
            {
                "pattern": r"rag_user_feedback.*JOIN.*learning_content",
                "issue": "Incorrect join: rag_user_feedback is for system feedback, not content feedback",
                "suggestion": "Use evaluation table for learning content feedback"
            },
            {
                "pattern": r"LIKE.*\|\|.*name.*\|\|",
                "issue": "Semantic mismatch: joining query text with content names",
                "suggestion": "Use proper foreign key relationships"
            }
        ]
    
    async def validate_and_correct(self, sql_query: str, original_question: str) -> Dict[str, Any]:
        """
        Validate SQL query and suggest corrections if needed.
        """
        issues = []
        for pattern_info in self.problematic_patterns:
            if re.search(pattern_info["pattern"], sql_query, re.IGNORECASE):
                issues.append(pattern_info)
        
        if issues:
            # Generate corrected query
            corrected_query = await self._generate_corrected_query(sql_query, original_question, issues)
            return {
                "valid": False,
                "issues": issues,
                "corrected_query": corrected_query,
                "should_retry": True
            }
        
        return {"valid": True, "issues": [], "should_retry": False}
```

#### **3.2 Integration with SQL Tool**
**File**: `src/rag/core/text_to_sql/sql_tool.py`

**Enhancement**: Add validation step before query execution

```python
async def execute_query_with_validation(self, query: str, original_question: str) -> SQLResult:
    """
    Execute SQL query with logical validation and auto-correction.
    """
    # Validate query logic
    validation_result = await self.query_validator.validate_and_correct(query, original_question)
    
    if not validation_result["valid"] and validation_result["should_retry"]:
        logger.warning(f"Query validation failed: {validation_result['issues']}")
        logger.info("Attempting query correction...")
        
        # Try corrected query
        corrected_query = validation_result["corrected_query"]
        return await self._execute_sql_query(corrected_query)
    
    return await self._execute_sql_query(query)
```

### **Phase 4: Enhanced Testing & Monitoring**

#### **4.1 Query Logic Test Suite**
**File**: `src/rag/tests/test_query_logic_validation.py` (new)

```python
class TestQueryLogicValidation:
    """
    Test suite for query logic validation and correction.
    """
    
    @pytest.mark.asyncio
    async def test_learning_content_feedback_queries(self):
        """Test that learning content feedback queries use correct tables."""
        test_cases = [
            "What feedback did users give about Live Learning courses?",
            "How do users feel about virtual training sessions?",
            "What are the main issues with online courses?",
            "Show me user comments about course quality"
        ]
        
        for question in test_cases:
            result = await self.sql_tool.process_question(question)
            
            # Should use evaluation table, not rag_user_feedback
            assert "evaluation" in result.query.lower()
            assert "rag_user_feedback" not in result.query.lower()
            assert result.row_count > 0  # Should return actual data
    
    @pytest.mark.asyncio
    async def test_rag_system_feedback_queries(self):
        """Test that RAG system feedback queries use correct tables."""
        test_cases = [
            "How do users rate the RAG system responses?",
            "What feedback do users give about system performance?",
            "Show me ratings for AI-generated answers"
        ]
        
        for question in test_cases:
            result = await self.sql_tool.process_question(question)
            
            # Should use rag_user_feedback table
            assert "rag_user_feedback" in result.query.lower()
            assert result.row_count >= 0  # May be empty, that's ok
```

#### **4.2 Real-time Query Monitoring**
**File**: `src/rag/utils/query_monitor.py` (new)

```python
class QueryLogicMonitor:
    """
    Monitors query generation for logic issues and tracks corrections.
    """
    
    async def log_query_analysis(self, question: str, generated_sql: str, result_count: int):
        """
        Log query generation analysis for continuous improvement.
        """
        analysis = {
            "timestamp": datetime.utcnow(),
            "question": question,
            "generated_sql": generated_sql,
            "result_count": result_count,
            "logic_issues": await self._detect_logic_issues(generated_sql),
            "table_usage": self._extract_table_usage(generated_sql)
        }
        
        # Alert on empty results for feedback queries
        if result_count == 0 and "feedback" in question.lower():
            await self._alert_empty_feedback_query(analysis)
```

---

## **Implementation Timeline**

### **Week 1: Core Fixes**
- [ ] Enhanced schema descriptions in SchemaManager
- [ ] Query validation layer implementation
- [ ] Updated SQL generation prompts

### **Week 2: Testing & Validation**
- [ ] Comprehensive test suite for query logic
- [ ] Black-box testing with corrected system
- [ ] Performance impact assessment

### **Week 3: Monitoring & Refinement**
- [ ] Real-time query monitoring implementation
- [ ] Feedback collection on query corrections
- [ ] Documentation updates

---

## **Expected Outcomes**

### **Immediate Benefits**
- ✅ Correct SQL generation for learning content feedback queries
- ✅ Proper data retrieval with meaningful results
- ✅ Improved answer synthesis quality

### **Long-term Benefits**
- ✅ Reduced query logic errors across all query types
- ✅ Better understanding of data model relationships
- ✅ Continuous improvement through monitoring and validation

### **Success Metrics**
- **Query Accuracy**: >95% of feedback queries use correct tables
- **Result Quality**: >90% of feedback queries return meaningful data
- **User Satisfaction**: Improved ratings for content feedback responses

---

## **Risk Assessment**

### **Low Risk**
- Enhanced schema descriptions (no breaking changes)
- Query validation layer (additive feature)

### **Medium Risk**
- Modified SQL generation prompts (requires thorough testing)
- Integration with existing query processing pipeline

### **Mitigation Strategies**
- Comprehensive test suite before deployment
- Gradual rollout with monitoring
- Fallback to original query generation if validation fails

---

## **Next Steps**

1. **Implement Phase 1** (Enhanced Schema Context) - Highest priority
2. **Test with original failing query** to validate fix
3. **Expand test cases** for comprehensive coverage
4. **Monitor query generation patterns** for other potential issues

This implementation plan addresses the root cause while building a robust framework for preventing similar issues in the future.