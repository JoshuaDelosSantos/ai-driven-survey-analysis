#!/usr/bin/env python3
"""
Manual Testing Script for Phase 1 RAG Refactoring

This script provides manual testing capabilities for the refactored RAG module.
It tests components with real or mock data to validate the Phase 1 implementation.

Usage:
    python src/rag/tests/manual_test_phase1.py [--mock] [--component COMPONENT]
    
Options:
    --mock        Use mock data instead of real API/DB calls
    --component   Test specific component: config, schema, sql, db, llm, logging, integration
"""

import sys
import asyncio
import argparse
from pathlib import Path
from typing import Optional

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent.parent))

print("üß™ RAG Phase 1 Manual Testing Suite")
print("=" * 50)


def test_configuration():
    """Test configuration loading and validation."""
    print("\nüìã Testing Configuration Management...")
    
    try:
        from rag.config.settings import get_settings, validate_configuration
        
        print("  ‚úì Importing configuration modules")
        
        # Test settings loading
        try:
            settings = get_settings()
            print("  ‚úì Settings loaded successfully")
            print(f"    Database: {settings.rag_db_host}:{settings.rag_db_port}")
            print(f"    Model: {settings.llm_model_name}")
        except Exception as e:
            print(f"  ‚ùå Settings loading failed: {e}")
            return False
        
        # Test configuration validation
        try:
            validate_configuration()
            print("  ‚úì Configuration validation passed")
            return True
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Configuration validation failed (expected with placeholder API key): {e}")
            print("  ‚ÑπÔ∏è  This is normal if you haven't set a real API key yet")
            return True  # Still consider successful for structure test
            
    except ImportError as e:
        print(f"  ‚ùå Import failed: {e}")
        return False


async def test_schema_manager(use_mock: bool = True):
    """Test schema manager functionality."""
    print("\nüóÇÔ∏è  Testing Schema Manager...")
    
    try:
        from rag.core.text_to_sql.schema_manager import SchemaManager, get_schema_for_llm
        from unittest.mock import Mock
        
        print("  ‚úì Importing schema manager")
        
        if use_mock:
            # Test with mock LLM
            mock_llm = Mock()
            mock_llm.invoke = Mock(return_value=Mock(content="Test SQL"))
            
            schema_manager = SchemaManager(llm=mock_llm)
            print("  ‚úì Schema manager created with mock LLM")
            
            # Test fallback schema
            fallback_schema = schema_manager._get_fallback_schema()
            print("  ‚úì Fallback schema generated")
            print(f"    Schema length: {len(fallback_schema)} characters")
            
            # Test table descriptions
            user_desc = schema_manager._get_table_description('users')
            print(f"  ‚úì Table description: {user_desc[:60]}...")
            
            # Test convenience function
            try:
                schema_description = await get_schema_for_llm(force_refresh=True)
                print("  ‚ö†Ô∏è  Schema generation succeeded (unexpected without DB)")
            except Exception as e:
                print("  ‚úì Schema generation properly failed without database connection")
            
            await schema_manager.close()
            print("  ‚úì Schema manager cleanup completed")
            
        return True
        
    except ImportError as e:
        print(f"  ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"  ‚ùå Test failed: {e}")
        return False


async def test_sql_tool(use_mock: bool = True):
    """Test async SQL tool functionality."""
    print("\nüîß Testing Async SQL Tool...")
    
    try:
        from rag.core.text_to_sql.sql_tool import AsyncSQLTool, SQLResult, query_database
        from unittest.mock import Mock
        
        print("  ‚úì Importing SQL tool")
        
        if use_mock:
            # Test with mock LLM
            mock_llm = Mock()
            mock_llm.invoke = Mock(return_value=Mock(content="SELECT COUNT(*) FROM users"))
            
            sql_tool = AsyncSQLTool(llm=mock_llm, max_retries=2)
            print("  ‚úì SQL tool created with mock LLM")
            
            # Test SQL extraction
            test_responses = [
                "```sql\nSELECT * FROM users\n```",
                "SQL Query: SELECT user_id FROM users",
                "SELECT agency, COUNT(*) FROM users GROUP BY agency"
            ]
            
            for response in test_responses:
                extracted = sql_tool._extract_sql_from_response(response)
                print(f"  ‚úì SQL extraction: '{response[:30]}...' ‚Üí '{extracted[:40]}...'")
            
            # Test SQL safety validation
            safe_queries = [
                "SELECT * FROM users",
                "SELECT COUNT(*) FROM attendance WHERE status = 'Completed'"
            ]
            
            unsafe_queries = [
                "DROP TABLE users",
                "INSERT INTO users VALUES (1, 'test')",
                "UPDATE users SET agency = 'test'"
            ]
            
            for query in safe_queries:
                assert sql_tool._is_safe_query(query) == True
                print(f"  ‚úì Safe query validated: {query[:40]}...")
            
            for query in unsafe_queries:
                assert sql_tool._is_safe_query(query) == False
                print(f"  ‚úì Unsafe query rejected: {query[:40]}...")
            
            # Test SQLResult creation
            result = SQLResult(
                query="SELECT COUNT(*) FROM users",
                result="count\n42",
                execution_time=0.123,
                success=True,
                row_count=1
            )
            print(f"  ‚úì SQLResult created: {result.success}, {result.execution_time}s")
            
            await sql_tool.close()
            print("  ‚úì SQL tool cleanup completed")
            
        return True
        
    except ImportError as e:
        print(f"  ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"  ‚ùå Test failed: {e}")
        return False


async def test_database_manager(use_mock: bool = True):
    """Test database manager functionality."""
    print("\nüóÑÔ∏è  Testing Database Manager...")
    
    try:
        from rag.utils.db_utils import DatabaseManager, get_database_manager
        
        print("  ‚úì Importing database manager")
        
        if use_mock:
            # Test with mock settings
            from unittest.mock import patch, Mock
            
            mock_settings = Mock(
                rag_db_host='localhost',
                rag_db_port=5432,
                rag_db_name='test_db',
                rag_db_user='test_user',
                rag_db_password='test_pass'
            )
            
            with patch('rag.utils.db_utils.get_settings', return_value=mock_settings):
                db_manager = DatabaseManager()
                print("  ‚úì Database manager created with mock settings")
                
                # Test read-only query validation
                test_queries = [
                    ("SELECT * FROM users", True, "Basic SELECT"),
                    ("WITH cte AS (SELECT * FROM users) SELECT * FROM cte", True, "CTE query"),
                    ("SELECT COUNT(*) FROM attendance", True, "Aggregate query"),
                    ("INSERT INTO users VALUES (1, 'test')", False, "Insert query"),
                    ("UPDATE users SET name = 'test'", False, "Update query"),
                    ("DELETE FROM users", False, "Delete query"),
                    ("DROP TABLE users", False, "DDL query")
                ]
                
                for query, expected, description in test_queries:
                    result = db_manager._is_readonly_query(query)
                    status = "‚úì" if result == expected else "‚ùå"
                    print(f"  {status} Query validation - {description}: {result}")
                
                await db_manager.close()
                print("  ‚úì Database manager cleanup completed")
        
        # Test global manager
        global_manager = get_database_manager()
        print("  ‚úì Global database manager retrieved")
        
        return True
        
    except ImportError as e:
        print(f"  ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"  ‚ùå Test failed: {e}")
        return False


def test_llm_manager(use_mock: bool = True):
    """Test LLM manager functionality."""
    print("\nü§ñ Testing LLM Manager...")
    
    try:
        from rag.utils.llm_utils import LLMManager, LLMResponse, get_llm_manager
        
        print("  ‚úì Importing LLM manager")
        
        if use_mock:
            from unittest.mock import patch, Mock
            
            mock_settings = Mock(
                llm_model_name='gpt-3.5-turbo',
                llm_api_key='test_key'
            )
            
            with patch('rag.utils.llm_utils.get_settings', return_value=mock_settings):
                with patch('rag.utils.llm_utils.ChatOpenAI') as mock_openai:
                    mock_llm_instance = Mock()
                    mock_openai.return_value = mock_llm_instance
                    
                    llm_manager = LLMManager()
                    print("  ‚úì LLM manager created with mock OpenAI")
                    
                    # Test Gemini LLM creation
                    gemini_manager = LLMManager(model_name='gemini-pro', api_key='test-key')
                    print("  ‚úì LLM manager created with mock Gemini")
                    
                    # Test example queries
                    examples = llm_manager.get_example_queries()
                    print(f"  ‚úì Example queries loaded: {len(examples)} examples")
                    
                    for i, example in enumerate(examples):
                        print(f"    Example {i+1}: {example['question'][:50]}...")
                    
                    # Test LLMResponse creation
                    response = LLMResponse(
                        content="SELECT COUNT(*) FROM users",
                        model="gpt-3.5-turbo",
                        tokens_used=25,
                        response_time=1.2,
                        success=True
                    )
                    print(f"  ‚úì LLMResponse created: {response.model}, {response.tokens_used} tokens")
        
        # Test global manager
        with patch('rag.utils.llm_utils.get_settings') as mock_settings:
            mock_settings.return_value = Mock(
                llm_model_name='gpt-3.5-turbo',
                llm_api_key='test_key'
            )
            with patch('rag.utils.llm_utils.ChatOpenAI'):
                global_manager = get_llm_manager()
                print("  ‚úì Global LLM manager retrieved")
        
        return True
        
    except ImportError as e:
        print(f"  ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"  ‚ùå Test failed: {e}")
        return False


def test_logging_utils():
    """Test logging utilities functionality."""
    print("\nüìù Testing Logging Utilities...")
    
    try:
        from rag.utils.logging_utils import (
            RAGLogger, PIIMaskingFormatter, get_logger, 
            mask_sensitive_data, log_event
        )
        
        print("  ‚úì Importing logging utilities")
        
        # Test PII masking
        formatter = PIIMaskingFormatter()
        
        test_messages = [
            ("Connection: postgresql://user:password@localhost:5432/db", "postgresql://***:***@"),
            ("API key: sk-1234567890abcdef1234567890abcdef", "sk-***"),
            ("Email: test.user@agency.gov.au", "***@***.***"),
            ("Phone: +61-412-345-678", "+***-***-****"),
            ("Normal message without PII", "Normal message without PII")
        ]
        
        for original, expected_pattern in test_messages:
            import logging
            logger = logging.getLogger('test_logger')
            record = logger.makeRecord(
                name='test_logger',
                level=logging.INFO,
                fn='test_file.py',
                lno=123,
                msg=original,
                args=(),
                exc_info=None
            )
            
            formatted = formatter.format(record)
            contains_pattern = expected_pattern in formatted
            status = "‚úì" if contains_pattern else "‚ùå"
            print(f"  {status} PII masking: '{original[:30]}...' ‚Üí contains '{expected_pattern}'")
        
        # Test mask_sensitive_data function
        sensitive_text = "API key sk-abcd1234 and email user@test.com"
        masked = mask_sensitive_data(sensitive_text)
        print(f"  ‚úì Sensitive data masking: '{sensitive_text}' ‚Üí '{masked}'")
        
        # Test logger creation
        from unittest.mock import patch, Mock
        mock_settings = Mock(log_level='INFO', debug_mode=False, log_file_path='/tmp/test_rag.log')
        
        with patch('rag.utils.logging_utils.get_settings', return_value=mock_settings):
            logger = get_logger('test_logger')
            print("  ‚úì RAG logger created")
            
            # Test log event function
            log_event('info', 'Test message', 'test_logger')
            print("  ‚úì Log event function called")
        
        return True
        
    except ImportError as e:
        print(f"  ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"  ‚ùå Test failed: {e}")
        return False


async def test_integration_flow(use_mock: bool = True):
    """Test integration between all components."""
    print("\nüîó Testing Integration Flow...")
    
    try:
        if use_mock:
            from unittest.mock import patch, Mock
            
            # Mock all external dependencies
            mock_settings = Mock(
                rag_db_host='localhost',
                rag_db_port=5432,
                rag_db_name='test_db',
                rag_db_user='test_user',
                rag_db_password='test_pass',
                llm_model_name='gpt-3.5-turbo',
                llm_api_key='test_key',
                log_level='INFO',
                debug_mode=False,
                get_database_uri=Mock(return_value='postgresql://test_user:test_pass@localhost:5432/test_db')
            )
            
            with patch('rag.config.settings.get_settings', return_value=mock_settings):
                with patch('rag.core.text_to_sql.schema_manager.get_settings', return_value=mock_settings):
                    with patch('rag.utils.db_utils.get_settings', return_value=mock_settings):
                        with patch('rag.utils.llm_utils.get_settings', return_value=mock_settings):
                            with patch('rag.utils.logging_utils.get_settings', return_value=mock_settings):
                                
                                print("  ‚úì All settings mocked")
                                
                                # Test configuration ‚Üí schema manager flow
                                from rag.config.settings import get_settings
                                from rag.core.text_to_sql.schema_manager import SchemaManager
                                
                                settings = get_settings()
                                print("  ‚úì Settings loaded in integration test")
                                
                                mock_llm = Mock()
                                mock_llm.invoke = Mock(return_value=Mock(content="SELECT * FROM users"))
                                
                                schema_manager = SchemaManager(llm=mock_llm)
                                print("  ‚úì Schema manager created in integration test")
                                
                                # Test schema ‚Üí SQL tool flow
                                from rag.core.text_to_sql.sql_tool import AsyncSQLTool
                                
                                sql_tool = AsyncSQLTool(llm=mock_llm)
                                print("  ‚úì SQL tool created in integration test")
                                
                                # Test that components can work together
                                fallback_schema = schema_manager._get_fallback_schema()
                                test_query = "How many users are there?"
                                
                                # Test SQL extraction
                                mock_response = "SELECT COUNT(*) FROM users"
                                extracted_sql = sql_tool._extract_sql_from_response(mock_response)
                                print(f"  ‚úì SQL extraction in integration: '{extracted_sql}'")
                                
                                # Test safety validation
                                is_safe = sql_tool._is_safe_query(extracted_sql)
                                print(f"  ‚úì SQL safety validation: {is_safe}")
                                
                                # Cleanup
                                await schema_manager.close()
                                await sql_tool.close()
                                print("  ‚úì Integration test cleanup completed")
        
        return True
        
    except ImportError as e:
        print(f"  ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"  ‚ùå Integration test failed: {e}")
        return False


async def test_async_signatures():
    """Test that all async functions are properly defined."""
    print("\n‚ö° Testing Async Function Signatures...")
    
    try:
        import inspect
        
        # Import modules to test
        from rag.core.text_to_sql.schema_manager import SchemaManager
        from rag.core.text_to_sql.sql_tool import AsyncSQLTool
        from rag.utils.db_utils import DatabaseManager
        
        print("  ‚úì All modules imported for async testing")
        
        # Test async methods
        async_methods = [
            (SchemaManager, 'get_database'),
            (SchemaManager, 'get_schema_description'),
            (AsyncSQLTool, 'generate_sql'),
            (AsyncSQLTool, 'execute_sql'),
            (AsyncSQLTool, 'process_question'),
            (DatabaseManager, 'execute_query'),
            (DatabaseManager, 'get_pool'),
        ]
        
        for cls, method_name in async_methods:
            method = getattr(cls, method_name)
            is_async = inspect.iscoroutinefunction(method)
            status = "‚úì" if is_async else "‚ùå"
            print(f"  {status} {cls.__name__}.{method_name} is async: {is_async}")
        
        print("  ‚úì Async signature validation completed")
        return True
        
    except ImportError as e:
        print(f"  ‚ùå Import failed: {e}")
        return False
    except Exception as e:
        print(f"  ‚ùå Async test failed: {e}")
        return False


def test_file_structure():
    """Test that all expected files exist."""
    print("\nüìÅ Testing File Structure...")
    
    expected_files = [
        'src/rag/__init__.py',
        'src/rag/runner.py',
        'src/rag/config/settings.py',
        'src/rag/core/text_to_sql/schema_manager.py',
        'src/rag/core/text_to_sql/sql_tool.py',
        'src/rag/utils/db_utils.py',
        'src/rag/utils/llm_utils.py',
        'src/rag/utils/logging_utils.py',
        'src/rag/interfaces/terminal_app.py',
        'src/rag/tests/test_phase1_refactoring.py'
    ]
    
    project_root = Path(__file__).parent.parent.parent.parent
    missing_files = []
    
    for file_path in expected_files:
        full_path = project_root / file_path
        if full_path.exists():
            print(f"  ‚úì {file_path}")
        else:
            print(f"  ‚ùå {file_path} (missing)")
            missing_files.append(file_path)
    
    if missing_files:
        print(f"  ‚ö†Ô∏è  {len(missing_files)} files missing")
        return False
    else:
        print("  ‚úì All expected files present")
        return True


async def main():
    """Main test runner."""
    parser = argparse.ArgumentParser(description='Manual test suite for Phase 1 RAG refactoring')
    parser.add_argument('--mock', action='store_true', help='Use mock data instead of real connections')
    parser.add_argument('--component', choices=['config', 'schema', 'sql', 'db', 'llm', 'logging', 'integration', 'async', 'files'], 
                       help='Test specific component only')
    
    args = parser.parse_args()
    use_mock = args.mock or True  # Default to mock for safety
    
    if use_mock:
        print("üé≠ Running in MOCK mode (safe for testing without real API keys)")
    else:
        print("üî¥ Running in LIVE mode (requires real API keys and database)")
    
    results = []
    
    # Run specific component test or all tests
    if args.component == 'config' or not args.component:
        results.append(('Configuration', test_configuration()))
    
    if args.component == 'schema' or not args.component:
        results.append(('Schema Manager', await test_schema_manager(use_mock)))
    
    if args.component == 'sql' or not args.component:
        results.append(('SQL Tool', await test_sql_tool(use_mock)))
    
    if args.component == 'db' or not args.component:
        results.append(('Database Manager', await test_database_manager(use_mock)))
    
    if args.component == 'llm' or not args.component:
        results.append(('LLM Manager', test_llm_manager(use_mock)))
    
    if args.component == 'logging' or not args.component:
        results.append(('Logging Utils', test_logging_utils()))
    
    if args.component == 'integration' or not args.component:
        results.append(('Integration Flow', await test_integration_flow(use_mock)))
    
    if args.component == 'async' or not args.component:
        results.append(('Async Signatures', await test_async_signatures()))
    
    if args.component == 'files' or not args.component:
        results.append(('File Structure', test_file_structure()))
    
    # Print summary
    print("\n" + "=" * 50)
    print("üìä TEST SUMMARY")
    print("=" * 50)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if result:
            passed += 1
    
    print(f"\nResults: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All tests passed! Phase 1 refactoring is working correctly.")
        print("\nüí° Next steps:")
        print("   1. Install dependencies: pip install -r requirements.txt")
        print("   2. Set up real API key in .env file")
        print("   3. Test with real database connection")
        print("   4. Run terminal application: python src/rag/runner.py")
    else:
        print("‚ö†Ô∏è  Some tests failed. Check the output above for details.")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
